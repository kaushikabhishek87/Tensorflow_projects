{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_text_generation_with_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYAkpeKTw8s1ahpHXdotA5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaushikabhishek87/Tensorflow_projects/blob/main/11_text_generation_with_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1LrFFMbhGvc",
        "outputId": "77ca6c02-98a7-497a-983f-34809014fce9"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-44833590-f4c6-6832-ac72-1c0ef91893da)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRaBaMwrlmr-"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl0RfaJel4dz",
        "outputId": "2aac12db-dddb-482c-ea64-c5f4e9ac4eb5"
      },
      "source": [
        "!wget https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-09 05:12:24--  https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.73.208, 142.250.73.240, 142.250.65.80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.73.208|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "\rshakespeare.txt       0%[                    ]       0  --.-KB/s               \rshakespeare.txt     100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-07-09 05:12:24 (110 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjmDWrGbnqxP"
      },
      "source": [
        "text = open(\"/content/shakespeare.txt\", mode=\"rb\" ).read().decode(encoding=\"utf8\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFOUNqUNBpOr"
      },
      "source": [
        "# text = open(\"/content/shakespeare.txt\", mode=\"r\" ).read()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7j-hlRQn0lj",
        "outputId": "91783e41-afd7-4f1e-a8ae-71b9295a2ede"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX5NF1YQoRVi",
        "outputId": "81df3ab1-d986-4ef1-d3f0-0818b8a9671e"
      },
      "source": [
        "# Unique characters \n",
        "vocab = set(text)\n",
        "print(len(vocab))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICCaeyZ5r713"
      },
      "source": [
        "## Preprocess Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnQbwxeXsim1",
        "outputId": "4131bfd9-3bb6-42c6-90d6-25a4e079c059"
      },
      "source": [
        "example_text = [\"there was just an earthquake\", \"hoping god everyone is safe\"]\n",
        "chars = tf.strings.unicode_split(example_text, input_encoding=\"UTF-8\")\n",
        "chars"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b't', b'h', b'e', b'r', b'e', b' ', b'w', b'a', b's', b' ', b'j', b'u', b's', b't', b' ', b'a', b'n', b' ', b'e', b'a', b'r', b't', b'h', b'q', b'u', b'a', b'k', b'e'], [b'h', b'o', b'p', b'i', b'n', b'g', b' ', b'g', b'o', b'd', b' ', b'e', b'v', b'e', b'r', b'y', b'o', b'n', b'e', b' ', b'i', b's', b' ', b's', b'a', b'f', b'e']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z7bntFFsxS4",
        "outputId": "10c85d1c-e2d2-4d62-d86f-3b7b9d38cf49"
      },
      "source": [
        "chars.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, None])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEQ9PdgftdBI"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab) , \n",
        "                                            mask_token=None)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjP9PRptCQKi",
        "outputId": "0785aade-b73b-4cfd-ce51-fba579168b5b"
      },
      "source": [
        "chars[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(28,), dtype=string, numpy=\n",
              "array([b't', b'h', b'e', b'r', b'e', b' ', b'w', b'a', b's', b' ', b'j',\n",
              "       b'u', b's', b't', b' ', b'a', b'n', b' ', b'e', b'a', b'r', b't',\n",
              "       b'h', b'q', b'u', b'a', b'k', b'e'], dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SwaCFKCtfzr",
        "outputId": "e6095fa4-102c-47c7-b117-248e10cb4944"
      },
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(28,), dtype=int64, numpy=\n",
              "array([54,  8, 32, 35, 32, 46, 64, 50, 21, 46,  4, 31, 21, 54, 46, 50, 62,\n",
              "       46, 32, 50, 35, 54,  8, 10, 31, 50, 18, 32])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVxD_yuMun8W",
        "outputId": "bcd1280c-87ba-4664-e2e5-520165d53a9c"
      },
      "source": [
        "chars_from_ids = preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(),\n",
        "                                            invert = True, mask_token=None)\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b't', b'h', b'e', b'r', b'e', b' ', b'w', b'a', b's', b' ', b'j', b'u', b's', b't', b' ', b'a', b'n', b' ', b'e', b'a', b'r', b't', b'h', b'q', b'u', b'a', b'k', b'e'], [b'h', b'o', b'p', b'i', b'n', b'g', b' ', b'g', b'o', b'd', b' ', b'e', b'v', b'e', b'r', b'y', b'o', b'n', b'e', b' ', b'i', b's', b' ', b's', b'a', b'f', b'e']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJnBVFUhvIWV",
        "outputId": "bebc6f4a-a6f8-409c-a0b1-2702294e0a7a"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'there was just an earthquake', b'hoping god everyone is safe'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_XvLz8RvbUk"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1).numpy()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0swHvkY0v0wE",
        "outputId": "5a83cae5-025a-4a3e-80f0-f3d09db4391d"
      },
      "source": [
        "text_from_ids(ids)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'there was just an earthquake', b'hoping god everyone is safe'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wbQDmuyv2tk"
      },
      "source": [
        "## Creating Training Exmaples & Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvDldFSuEjwv",
        "outputId": "97ffbb50-574e-4ca9-c1cd-aa3196a67be7"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, input_encoding=\"UTF-8\"))\n",
        "all_ids"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([ 2, 27, 35, ..., 15, 56, 25])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLAJDRmCE8nQ",
        "outputId": "ab2044a3-49b1-45fc-85aa-72f96b7c98e0"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "ids_dataset"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fAUDzPMFfV2",
        "outputId": "47a00e0e-abf9-46f8-d682-f5e336570682"
      },
      "source": [
        "for i in ids_dataset.take(10):\n",
        "  print(chars_from_ids(i).numpy().decode(\"UTF-8\") )\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGQ_5b4qFo04",
        "outputId": "62eeb2db-9eb3-4de2-9ef7-bb6c5931611e"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "examples_per_epoch"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11043"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBjysZGeGWgj",
        "outputId": "4f117c2a-a5a0-46a7-ac68-4dded327617e"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for i in sequences.take(1):\n",
        "  print(chars_from_ids(i))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoASYlkUGYTC",
        "outputId": "c9d23340-8400-4f80-e663-9fd59c08db4c"
      },
      "source": [
        "for i in sequences.take(5):\n",
        "  print(text_from_ids(i))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_uOogvXIM1s"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLfvxYHjKCZe",
        "outputId": "01c4dc7f-3251-4db4-8e10-08e144c1e397"
      },
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD91Y-g-KLIw"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlE_M5B0KjHh",
        "outputId": "1af0c7c6-574c-4551-e256-575b292079bb"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(text_from_ids(input_example))\n",
        "  print(text_from_ids(target_example))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3Td_L60KxQP"
      },
      "source": [
        "## Creating training batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR3KDEMdLJHZ",
        "outputId": "c4dd72c7-3ce8-4470-93c1-c8cc1cac5038"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "dataset"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM0fFvJILuIF",
        "outputId": "92bac560-2d23-4ba4-d152-3b0aa51927d2"
      },
      "source": [
        "dataset.take(1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JKpKOw_L5TQ"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfvTq_VsrqJX"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwq3phKMr7eP"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYxVEQbhr9lv",
        "outputId": "2ce205e4-4c2c-4f8f-bb09-0675ed5fe892"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL9xQar4r_xJ",
        "outputId": "2a155a97-828b-4131-8181-af66cb966f08"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      multiple                  16896     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  67650     \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCPALhRXsCBh"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APsUMWs4sE-t",
        "outputId": "55595929-b0a6-43e6-f6f3-538969074bd1"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([58, 31,  7, 42, 44, 29,  9, 45, 39, 41,  2, 58,  6, 28, 12, 23, 36,\n",
              "       22, 35,  8, 56,  8, 63, 40, 41,  0, 33, 16, 63, 39, 62, 14, 30, 30,\n",
              "       32, 63, 33, 29, 37, 29, 64, 56, 10,  2, 43, 51, 10, 48,  9, 39, 22,\n",
              "       61, 22, 64, 57, 16, 61, 35,  0, 38, 32, 53, 37, 19, 22, 59, 11, 14,\n",
              "       52, 34, 40, 34, 28, 32, 25,  9, 57, 43, 55,  7,  9, 51, 20, 54, 39,\n",
              "       27, 28, 48, 44, 47, 48, 35, 55,  1, 42, 34,  1, 29, 39, 60])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "5u6WCSClsHK-",
        "outputId": "bda9189a-8ccb-410d-d9f7-b4378f6e2743"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-ca14a90865dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_from_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_example_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Next Char Predictions:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_from_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'numpy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giVaC9TjsJHT"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViiM_CJTsO4S",
        "outputId": "20f1494b-3af1-41a0-acc7-8cd7510b81e3"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.189619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A2QDqbjsQph",
        "outputId": "12062908-fec2-4fa9-a041-b9c24d22ff57"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.99765"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsq43IQisTwz"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPJPazVRsVPM",
        "outputId": "cdb93f59-85fb-4f73-b25d-4c375c6e35f0"
      },
      "source": [
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 2.6684\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9777\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.7175\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 1.5508\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 10s 58ms/step - loss: 1.4490\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 10s 59ms/step - loss: 1.3792\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 10s 58ms/step - loss: 1.3253\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 1.2803\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 1.2393\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 1.1989\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 1.1572\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 1.1161\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 1.0710\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 1.0235\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 0.9754\n",
            "Epoch 16/20\n",
            " 30/172 [====>.........................] - ETA: 8s - loss: 0.9507"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsOhBI8Fsaa8"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtpM3qgxtw9c"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEnbBf_1tq4h",
        "outputId": "90e6488d-95a0-4d1f-9482-f0f2556cd9e2"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "That she's the rest.\n",
            "\n",
            "ANTONIO:\n",
            "It may be found.\n",
            "\n",
            "LUCENTIO:\n",
            "I pray thee?\n",
            "\n",
            "ADRIAN:\n",
            "Tut, that's my sister, take my rain'd? let's all\n",
            "enough, I would fain have oftence and command against\n",
            "The cause of Tybalt hate drew o' no\n",
            "man get apart, against the swaggars, we can: for,\n",
            "As far mar in a bue thought our marriage\n",
            "With a sweeter thanks for abuses!\n",
            "\n",
            "Provos:\n",
            "Here, sir; a dozen way, I say; for then he consents\n",
            "Her subsed from the beggary he were before.\n",
            "\n",
            "ARIEL:\n",
            "My lord, my leave me with thy glory lie;\n",
            "But think the white beauty cell, and help your ears.\n",
            "\n",
            "HORTENSIO:\n",
            "Sirrah, brave Messenger:\n",
            "What mounted liberty, sir? what comes her?\n",
            "\n",
            "GREMIO:\n",
            "It shall we tell thee, was the duke and know;\n",
            "With crief, old desert did but bodies,--for the plained of\n",
            "my hoise-vorced being wed, in the right over eye's great\n",
            "Appointed better through the city love.\n",
            "First, nor no more stand by: herein our fresh,\n",
            "jestiors, the beggary know not your suffer and my strong\n",
            "Masters of selves, our mosts and wear shoulders\n",
            "shac \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.2077980041503906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t85TxZGtx3K"
      },
      "source": [
        "## Keras API Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4KAk7fswcm9",
        "outputId": "bb9208f0-a4f7-4847-9b7e-71e773175c43"
      },
      "source": [
        "dataset.take(1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H8nJbQUv1p1",
        "outputId": "c8f8f64c-6fb5-42d1-c2cc-f368e153e9aa"
      },
      "source": [
        "embedding = tf.keras.layers.Embedding(input_dim=len(ids_from_chars.get_vocabulary())+1, output_dim= embedding_dim)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape = (100,) )\n",
        "X = embedding(inputs)\n",
        "X = tf.keras.layers.GRU(units=rnn_units, return_sequences=False, return_state= False )(X)\n",
        "outputs = tf.keras.layers.Dense(units=len(ids_from_chars.get_vocabulary()) , activation=\"relu\")(X)\n",
        "\n",
        "model_seq = tf.keras.models.Model(inputs, outputs)\n",
        "\n",
        "model_seq.summary()\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 100, 256)          17152     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 1024)              3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 66)                67650     \n",
            "=================================================================\n",
            "Total params: 4,023,106\n",
            "Trainable params: 4,023,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih5gLFC5xw3Z"
      },
      "source": [
        "model_seq."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}